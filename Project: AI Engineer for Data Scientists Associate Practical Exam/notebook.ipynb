{"cells":[{"source":"# Practical Exam: Customer Purchase Prediction\n\nRetailTech Solutions is a fast-growing international e-commerce platform operating in over 20 countries across Europe, North America, and Asia. They specialize in fashion, electronics, and home goods, with a unique business model that combines traditional retail with a marketplace for independent sellers.\n\nThe company has seen rapid growth. A key part of their success has been their data-driven approach to personalization. However, as they plan their expansion into new markets, they need to improve their ability to predict customer behavior.\n\nTheir marketing team wants to predict which customers are most likely to make a purchase based on their browsing behavior.\n\nAs an AI Engineer, you will help build this prediction system. Your work will directly impact RetailTech's growth strategy and their goal of increasing revenue.\n\n\n## Data Description\n\n| Column Name | Criteria |\n|------------|----------|\n| customer_id | Integer. Unique identifier for each customer. No missing values. |\n| time_spent | Float. Minutes spent on website per session. Missing values should be replaced with median. |\n| pages_viewed | Integer. Number of pages viewed in session. Missing values should be replaced with mean. |\n| basket_value | Float. Value of items in basket. Missing values should be replaced with 0. |\n| device_type | String. One of: Mobile, Desktop, Tablet. Missing values should be replaced with \"Unknown\". |\n| customer_type | String. One of: New, Returning. Missing values should be replaced with \"New\". |\n| purchase | Binary. Whether customer made a purchase (1) or not (0). Target variable. |","metadata":{},"id":"8d0bcede-0826-475c-8678-72835c042b37","cell_type":"markdown"},{"source":"# Task 1\n\nThe marketing team has collected customer session data in `raw_customer_data.csv`, but it contains missing values and inconsistencies that need to be addressed.\nCreate a cleaned version of the dataframe:\n\n- Start with the data in the file `raw_customer_data.csv`\n- Your output should be a DataFrame named `clean_data`\n- All column names and values should match the table below.\n</br>\n\n| Column Name | Criteria |\n|------------|----------|\n| customer_id | Integer. Unique identifier for each customer. No missing values. |\n| time_spent | Float. Minutes spent on website per session. Missing values should be replaced with median. |\n| pages_viewed | Integer. Number of pages viewed in session. Missing values should be replaced with mean. |\n| basket_value | Float. Value of items in basket. Missing values should be replaced with 0. |\n| device_type | String. One of: Mobile, Desktop, Tablet. Missing values should be replaced with \"Unknown\". |\n| customer_type | String. One of: New, Returning. Missing values should be replaced with \"New\". |\n| purchase | Binary. Whether customer made a purchase (1) or not (0). Target variable. |","metadata":{},"id":"c0d5a3bb-bbae-4d39-a6c6-daa46c470347","cell_type":"markdown"},{"source":"# Write your answer to Task 1 here \nimport pandas as pd\n\n# Read the data into a pandas DataFrame\nraw_data = pd.read_csv('raw_customer_data.csv')\n\n# Create a copy to hold the cleaned data\nclean_data = raw_data.copy()\n\n# Fill missing 'time_spent' values with the median\ntime_spent_median = clean_data['time_spent'].median()\nclean_data['time_spent'].fillna(time_spent_median, inplace=True)\n\n# Fill missing 'pages_viewed' values with the mean and cast to integer\npages_viewed_mean = clean_data['pages_viewed'].mean()\nclean_data['pages_viewed'].fillna(pages_viewed_mean, inplace=True)\nclean_data['pages_viewed'] = clean_data['pages_viewed'].astype(int)\n\n# Fill missing 'basket_value' values with 0\nclean_data['basket_value'].fillna(0, inplace=True)\n\n# Fill missing 'device_type' values with 'Unknown'\nclean_data['device_type'].fillna(\"Unknown\", inplace=True)\n\n# Fill missing 'customer_type' values with 'New'\nclean_data['customer_type'].fillna(\"New\", inplace=True)\n\nprint(clean_data.info())\nprint(clean_data.head())","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":519,"type":"stream"}}},"id":"5ce18b54-29af-4beb-bc8c-79c4e21bcd52","cell_type":"code","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 500 entries, 0 to 499\nData columns (total 7 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   customer_id    500 non-null    int64  \n 1   time_spent     500 non-null    float64\n 2   pages_viewed   500 non-null    int64  \n 3   basket_value   500 non-null    float64\n 4   device_type    500 non-null    object \n 5   customer_type  500 non-null    object \n 6   purchase       500 non-null    int64  \ndtypes: float64(2), int64(3), object(2)\nmemory usage: 27.5+ KB\nNone\n   customer_id  time_spent  pages_viewed  ...  device_type customer_type purchase\n0            1   23.097867             7  ...       Mobile     Returning        0\n1            2   57.092144             3  ...       Mobile     Returning        1\n2            3   44.187643            14  ...       Mobile     Returning        0\n3            4   36.320851            10  ...       Mobile           New        1\n4            5   10.205100            16  ...       Mobile     Returning        1\n\n[5 rows x 7 columns]\n"}]},{"source":"# Task 2\nThe pre-cleaned dataset `model_data.csv` needs to be prepared for our neural network.\nCreate the model features:\n\n- Start with the data in the file `model_data.csv`\n- Scale numerical features (`time_spent`, `pages_viewed`, `basket_value`) to 0-1 range\n- Apply one-hot encoding to the categorical features (`device_type`, `customer_type`)\n    - The column names should have the following format: variable_name_category_name (e.g., `device_type_Desktop`)\n- Your output should be a DataFrame named `model_feature_set`, with all column names from `model_data.csv` except for the columns where one-hot encoding was applied.\n","metadata":{},"id":"026b3c30-d3b0-4762-ae10-0f2880873bdc","cell_type":"markdown"},{"source":"# Write your answer to Task 2 here\nfrom sklearn.preprocessing import MinMaxScaler\nmodel_data = pd.read_csv('model_data.csv')\n\n# Isolate numerical and categorical features for transformation\nnumerical_features = ['time_spent', 'pages_viewed', 'basket_value']\ncategorical_features = ['device_type', 'customer_type']\n\n# Initialize the MinMaxScaler\nscaler = MinMaxScaler()\n\n# Create a copy of the original data to modify\nmodel_feature_set = model_data.copy()\n\n# Scale numerical features and update the DataFrame\nmodel_feature_set[numerical_features] = scaler.fit_transform(model_data[numerical_features])\n\n# Apply one-hot encoding and get the new encoded columns\nencoded_df = pd.get_dummies(model_data[categorical_features], prefix=categorical_features, prefix_sep='_')\n\n# Combine the original DataFrame (with scaled numerical columns) with the new encoded columns\nmodel_feature_set = pd.concat([model_feature_set, encoded_df], axis=1)\n\n# Drop the original categorical columns as they are now encoded\nmodel_feature_set.drop(columns=categorical_features, inplace=True)\n\n# The 'model_feature_set' DataFrame is now ready for the neural network.\nprint(model_feature_set.head())\nprint(model_feature_set.info())\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":606,"type":"stream"}}},"id":"6d47e440-c4ab-45cf-af40-53181764bac4","cell_type":"code","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":"   customer_id  time_spent  ...  customer_type_New  customer_type_Returning\n0          501    0.664167  ...                  1                        0\n1          502    0.483681  ...                  0                        1\n2          503    0.231359  ...                  0                        1\n3          504    0.792944  ...                  1                        0\n4          505    0.649210  ...                  1                        0\n\n[5 rows x 11 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 500 entries, 0 to 499\nData columns (total 11 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   customer_id              500 non-null    int64  \n 1   time_spent               500 non-null    float64\n 2   pages_viewed             500 non-null    float64\n 3   basket_value             500 non-null    float64\n 4   purchase                 500 non-null    int64  \n 5   device_type_Desktop      500 non-null    uint8  \n 6   device_type_Mobile       500 non-null    uint8  \n 7   device_type_Tablet       500 non-null    uint8  \n 8   device_type_Unknown      500 non-null    uint8  \n 9   customer_type_New        500 non-null    uint8  \n 10  customer_type_Returning  500 non-null    uint8  \ndtypes: float64(3), int64(2), uint8(6)\nmemory usage: 22.6 KB\nNone\n"}]},{"source":"# Task 3\n\nNow that all preparatory work has been done, create and train a neural network that would allow the company to predict purchases.\n\n- Using PyTorch, create a network with:\n   - At least one hidden layer with 8 units\n   - ReLU activation for hidden layer\n   - Sigmoid activation for the output layer\n- Using the prepared features in `input_model_features.csv`, train the model to predict purchases. \n- Use the validation dataset `validation_features.csv` to predict new values based on the trained model. \n- Your model should be named `purchase_model` and your output should be a DataFrame named `validation_predictions` with columns `customer_id` and `purchase`. The `purchase` column must be your predicted values.\n","metadata":{},"id":"10a02327-d528-441c-87bf-098f9d6415e1","cell_type":"markdown"},{"source":"# Write your answer to Task 3 here\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ninput_model_features = pd.read_csv('input_model_features.csv')\nvalidation_features = pd.read_csv('validation_features.csv')\n\n# Identify feature columns (all columns except identifiers and the target)\nfeature_columns = [col for col in full_feature_set.columns if col not in ['customer_id', 'purchase']]\n\n# Prepare training data tensors\nX_train = input_model_features[feature_columns].values\ny_train = input_model_features['purchase'].values.reshape(-1, 1)\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n\n# Prepare validation data tensors\nX_val = validation_features[feature_columns].values\nvalidation_customer_ids = validation_features['customer_id']\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n\n# --- 2. Neural Network Definition ---\n\nclass PurchasePredictorNet(nn.Module):\n    def __init__(self, input_size):\n        super(PurchasePredictorNet, self).__init__()\n        # First hidden layer\n        self.layer1 = nn.Linear(input_size, 8)\n        self.relu = nn.ReLU()\n        # Output layer\n        self.output_layer = nn.Linear(8, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.output_layer(x)\n        x = self.sigmoid(x)\n        return x\n\n# --- 3. Model Training ---\n\n# Set a random seed for reproducible results\ntorch.manual_seed(42)\n\n# Instantiate the model\ninput_size = len(feature_columns)\npurchase_model = PurchasePredictorNet(input_size)\n\n# Define the loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(purchase_model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 500\nfor epoch in range(epochs):\n    purchase_model.train()\n    \n    # Forward pass: compute predicted y by passing x to the model\n    outputs = purchase_model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    \n    # Zero gradients, perform a backward pass, and update the weights\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n# --- 4. Prediction on Validation Set ---\n\n# Set model to evaluation mode (disables dropout, etc.)\npurchase_model.eval()\nwith torch.no_grad(): # Disable gradient computation\n    # Get raw predictions (probabilities)\n    raw_predictions = purchase_model(X_val_tensor)\n    # Convert probabilities to binary classes (0 or 1) based on a 0.5 threshold\n    predicted_classes = (raw_predictions > 0.5).int()\n\n# --- 5. Create Output DataFrame ---\n\nvalidation_predictions = pd.DataFrame({\n    'customer_id': validation_customer_ids.values,\n    'purchase': predicted_classes.numpy().flatten()\n})\n\nprint(\"Validation Predictions:\")\nprint(validation_predictions)","metadata":{"executionCancelledAt":null,"executionTime":1294,"lastExecutedAt":1750957907584,"lastExecutedByKernel":"217eb46c-d837-454c-bbe7-34797addf396","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Write your answer to Task 3 here\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ninput_model_features = pd.read_csv('input_model_features.csv')\nvalidation_features = pd.read_csv('validation_features.csv')\n\n# Identify feature columns (all columns except identifiers and the target)\nfeature_columns = [col for col in full_feature_set.columns if col not in ['customer_id', 'purchase']]\n\n# Prepare training data tensors\nX_train = input_model_features[feature_columns].values\ny_train = input_model_features['purchase'].values.reshape(-1, 1)\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n\n# Prepare validation data tensors\nX_val = validation_features[feature_columns].values\nvalidation_customer_ids = validation_features['customer_id']\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n\n# --- 2. Neural Network Definition ---\n\nclass PurchasePredictorNet(nn.Module):\n    def __init__(self, input_size):\n        super(PurchasePredictorNet, self).__init__()\n        # First hidden layer\n        self.layer1 = nn.Linear(input_size, 8)\n        self.relu = nn.ReLU()\n        # Output layer\n        self.output_layer = nn.Linear(8, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.output_layer(x)\n        x = self.sigmoid(x)\n        return x\n\n# --- 3. Model Training ---\n\n# Set a random seed for reproducible results\ntorch.manual_seed(42)\n\n# Instantiate the model\ninput_size = len(feature_columns)\npurchase_model = PurchasePredictorNet(input_size)\n\n# Define the loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(purchase_model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 500\nfor epoch in range(epochs):\n    purchase_model.train()\n    \n    # Forward pass: compute predicted y by passing x to the model\n    outputs = purchase_model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    \n    # Zero gradients, perform a backward pass, and update the weights\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n# --- 4. Prediction on Validation Set ---\n\n# Set model to evaluation mode (disables dropout, etc.)\npurchase_model.eval()\nwith torch.no_grad(): # Disable gradient computation\n    # Get raw predictions (probabilities)\n    raw_predictions = purchase_model(X_val_tensor)\n    # Convert probabilities to binary classes (0 or 1) based on a 0.5 threshold\n    predicted_classes = (raw_predictions > 0.5).int()\n\n# --- 5. Create Output DataFrame ---\n\nvalidation_predictions = pd.DataFrame({\n    'customer_id': validation_customer_ids.values,\n    'purchase': predicted_classes.numpy().flatten()\n})\n\nprint(\"Validation Predictions:\")\nprint(validation_predictions)","outputsMetadata":{"0":{"height":344,"type":"stream"}}},"id":"efcbda28-3c89-480d-b77a-c7f27ac759d5","cell_type":"code","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":"Validation Predictions:\n     customer_id  purchase\n0           1801         1\n1           1802         1\n2           1803         1\n3           1804         0\n4           1805         1\n..           ...       ...\n195         1996         1\n196         1997         1\n197         1998         1\n198         1999         1\n199         2000         1\n\n[200 rows x 2 columns]\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}